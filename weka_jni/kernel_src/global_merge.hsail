version 0:20140528:$full:$large;
extension "amd:gcn";
extension "IMAGE";

decl prog function &abort()();

prog kernel &__OpenCL_run_kernel(
	kernarg_u64 %global_offset_0,
	kernarg_u64 %global_offset_1,
	kernarg_u64 %global_offset_2,
	kernarg_u64 %printf_buffer,
	kernarg_u64 %vqueue_pointer,
	kernarg_u64 %aqlwrap_pointer,
	kernarg_u32 %srcVecSize,
	kernarg_u32 %srcLogicalBlockSize,
	kernarg_u64 %source_iter,
	kernarg_u64 %result_iter,
	kernarg_u64 %src_offsets,
	kernarg_u64 %result_offsets)
{
	pragma  "AMD RTI", "ARGSTART:__OpenCL_run_kernel";
	pragma  "AMD RTI", "version:3:1:104";
	pragma  "AMD RTI", "device:generic";
	pragma  "AMD RTI", "uniqueid:1027";
	pragma  "AMD RTI", "memory:private:0";
	pragma  "AMD RTI", "memory:region:0";
	pragma  "AMD RTI", "memory:local:0";
	pragma  "AMD RTI", "value:global_offset_0:u64:1:1:0";
	pragma  "AMD RTI", "value:global_offset_1:u64:1:1:16";
	pragma  "AMD RTI", "value:global_offset_2:u64:1:1:32";
	pragma  "AMD RTI", "pointer:printf_buffer:u8:1:1:48:uav:8:1:RW:0:0:0";
	pragma  "AMD RTI", "value:vqueue_pointer:u64:1:1:64";
	pragma  "AMD RTI", "value:aqlwrap_pointer:u64:1:1:80";
	pragma  "AMD RTI", "value:srcVecSize:u32:1:1:96";
	pragma  "AMD RTI", "value:srcLogicalBlockSize:u32:1:1:112";
	pragma  "AMD RTI", "pointer:source_iter:double:1:1:128:uav:8:8:RW:0:0:0";
	pragma  "AMD RTI", "pointer:result_iter:double:1:1:144:uav:8:8:RW:0:0:0";
	pragma  "AMD RTI", "pointer:src_offsets:u32:1:1:160:uav:8:4:RW:0:0:0";
	pragma  "AMD RTI", "pointer:result_offsets:u32:1:1:176:uav:8:4:RW:0:0:0";
	pragma  "AMD RTI", "function:1:0";
	pragma  "AMD RTI", "memory:64bitABI";
	pragma  "AMD RTI", "uavid:8";
	pragma  "AMD RTI", "privateid:8";
	pragma  "AMD RTI", "enqueue_kernel:0";
	pragma  "AMD RTI", "kernel_index:0";
	pragma  "AMD RTI", "reflection:0:size_t";
	pragma  "AMD RTI", "reflection:1:size_t";
	pragma  "AMD RTI", "reflection:2:size_t";
	pragma  "AMD RTI", "reflection:3:size_t";
	pragma  "AMD RTI", "reflection:4:size_t";
	pragma  "AMD RTI", "reflection:5:size_t";
	pragma  "AMD RTI", "reflection:6:uint";
	pragma  "AMD RTI", "reflection:7:uint";
	pragma  "AMD RTI", "reflection:8:double*";
	pragma  "AMD RTI", "reflection:9:double*";
	pragma  "AMD RTI", "reflection:10:uint*";
	pragma  "AMD RTI", "reflection:11:uint*";
	pragma  "AMD RTI", "ARGEND:__OpenCL_run_kernel";

@__OpenCL_run_kernel_entry:
	// BB#0:                                // %entry
	workitemabsid_u32	$s0, 0;
	cvt_u64_u32	$d0, $s0;
	ld_kernarg_align(8)_width(all)_u64	$d1, [0];
	add_u64	$d0, $d0, $d1;
	ld_kernarg_align(4)_width(all)_u32	$s4, [%srcVecSize];
	cvt_u64_u32	$d1, $s4;
	cmp_ge_b1_u64	$c0, $d0, $d1;
	cbr_b1	$c0, @BB0_14;
	// BB#1:                                // %if.end
	ld_kernarg_align(8)_width(all)_u64	$d2, [%source_iter];
	ld_kernarg_align(4)_width(all)_u32	$s2, [%srcLogicalBlockSize];
	cvt_u64_u32	$d3, $s2;
	div_u64	$d1, $d0, $d3;
	and_b64	$d4, $d1, 1;
	cvt_b1_u64	$c0, $d4;
	cmov_b32	$s1, $c0, 0, $s2;
	shl_u32	$s0, $s2, 1;
	neg_s32	$s3, $s0;
	cvt_u64_u32	$d4, $s3;
	and_b64	$d4, $d0, $d4;
	cvt_u32_u64	$s3, $d4;
	add_u32	$s1, $s1, $s3;
	min_u32	$s1, $s1, $s4;
	add_u32	$s5, $s1, $s2;
	rem_u64	$d3, $d0, $d3;
	cvt_u32_u64	$s2, $d3;
	cvt_u32_u64	$s3, $d1;
	shl_u64	$d1, $d0, 3;
	add_u64	$d1, $d2, $d1;
	min_u32	$s5, $s5, $s4;
	ld_global_align(8)_f64	$d1, [$d1];
	mov_b32	$s4, $s1;
	not_b1	$c0, $c0;
	cbr_b1	$c0, @BB0_2;
	// BB#5:
	mov_b32	$s6, $s5;
	br	@BB0_6;

@BB0_8:
	// %if.then.i.i
	add_u32	$s4, $s6, 1;
	mov_b32	$s6, $s7;

@BB0_6:
	// %for.cond.i.i
	mov_b32	$s7, $s6;
	cmp_ge_b1_u32	$c0, $s4, $s7;
	cbr_b1	$c0, @BB0_9;
	// BB#7:                                // %for.body.i.i
	add_u32	$s6, $s7, $s4;
	shr_u32	$s6, $s6, 1;
	cvt_u64_u32	$d3, $s6;
	shl_u64	$d3, $d3, 3;
	add_u64	$d3, $d2, $d3;
	ld_global_align(8)_f64	$d3, [$d3];
	cmp_geu_b1_f64	$c0, $d3, $d1;
	cbr_b1	$c0, @BB0_6;
	br	@BB0_8;

@BB0_9:
	// %lowerBoundBinary.exit.i
	cmp_ne_b1_s32	$c0, $s4, $s5;
	cbr_b1	$c0, @BB0_11;
	// BB#10:
	mov_b32	$s4, $s5;
	br	@BB0_13;

@BB0_4:
	// %if.then.i
	add_u32	$s4, $s5, 1;
	mov_b32	$s5, $s6;

@BB0_2:
	// %for.cond.i
	mov_b32	$s6, $s5;
	cmp_ge_b1_u32	$c0, $s4, $s6;
	cbr_b1	$c0, @BB0_13;
	// BB#3:                                // %for.body.i
	add_u32	$s5, $s6, $s4;
	shr_u32	$s5, $s5, 1;
	cvt_u64_u32	$d3, $s5;
	shl_u64	$d3, $d3, 3;
	add_u64	$d3, $d2, $d3;
	ld_global_align(8)_f64	$d3, [$d3];
	cmp_geu_b1_f64	$c0, $d3, $d1;
	cbr_b1	$c0, @BB0_2;
	br	@BB0_4;

@BB0_11:
	// %for.cond.preheader.i
	cvt_u64_u32	$d3, $s4;
	shl_u64	$d3, $d3, 3;
	add_u64	$d3, $d2, $d3;
	ld_global_align(8)_f64	$d3, [$d3];
	cmp_lt_b1_u32	$c0, $s4, $s5;
	cmp_eq_b1_f64	$c1, $d3, $d1;
	and_b1	$c0, $c1, $c0;
	cmp_ne_b1_b1	$c0, $c0, 1;
	cbr_b1	$c0, @BB0_13;

@BB0_12:
	// %for.body.i5
	add_u32	$s6, $s4, $s5;
	shr_u32	$s6, $s6, 1;
	cvt_u64_u32	$d3, $s6;
	shl_u64	$d3, $d3, 3;
	add_u64	$d3, $d2, $d3;
	ld_global_align(8)_f64	$d3, [$d3];
	cmp_eq_b1_f64	$c0, $d3, $d1;
	cmov_b32	$s5, $c0, $s5, $s6;
	cmov_b32	$s4, $c0, $s6, $s4;
	add_u32	$s4, $s4, 1;
	cvt_u64_u32	$d3, $s4;
	shl_u64	$d3, $d3, 3;
	add_u64	$d3, $d2, $d3;
	ld_global_align(8)_f64	$d3, [$d3];
	cmp_lt_b1_u32	$c0, $s4, $s5;
	cmp_eq_b1_f64	$c1, $d3, $d1;
	and_b1	$c0, $c1, $c0;
	cbr_b1	$c0, @BB0_12;

@BB0_13:
	// %if.end34
	ld_kernarg_align(8)_width(all)_u64	$d4, [%result_iter];
	ld_kernarg_align(8)_width(all)_u64	$d2, [%src_offsets];
	ld_kernarg_align(8)_width(all)_u64	$d3, [%result_offsets];
	shr_u32	$s3, $s3, 1;
	mad_u32	$s0, $s3, $s0, $s2;
	sub_u32	$s0, $s0, $s1;
	add_u32	$s0, $s0, $s4;
	cvt_u64_u32	$d5, $s0;
	shl_u64	$d6, $d5, 3;
	add_u64	$d4, $d4, $d6;
	st_global_align(8)_f64	$d1, [$d4];
	shl_u64	$d1, $d5, 2;
	add_u64	$d1, $d3, $d1;
	shl_u64	$d0, $d0, 2;
	add_u64	$d0, $d2, $d0;
	ld_global_align(4)_u32	$s0, [$d0];
	st_global_align(4)_u32	$s0, [$d1];

@BB0_14:
	// %return
	ret;
};
