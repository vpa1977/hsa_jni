version 0:20140528:$full:$large;
extension "amd:gcn";
extension "IMAGE";

decl prog function &abort()();

prog kernel &__OpenCL_run_kernel(
	kernarg_u64 %global_offset_0,
	kernarg_u64 %global_offset_1,
	kernarg_u64 %global_offset_2,
	kernarg_u64 %printf_buffer,
	kernarg_u64 %vqueue_pointer,
	kernarg_u64 %aqlwrap_pointer,
	kernarg_u64 %input,
	kernarg_u32 %length,
	kernarg_u32 %stride,
	kernarg_u32 %offset,
	kernarg_u64 %result)
{
	pragma  "AMD RTI", "ARGSTART:__OpenCL_run_kernel";
	pragma  "AMD RTI", "version:3:1:104";
	pragma  "AMD RTI", "device:generic";
	pragma  "AMD RTI", "uniqueid:1032";
	pragma  "AMD RTI", "memory:private:0";
	pragma  "AMD RTI", "memory:region:0";
	pragma  "AMD RTI", "memory:local:3072";
	pragma  "AMD RTI", "value:global_offset_0:u64:1:1:0";
	pragma  "AMD RTI", "value:global_offset_1:u64:1:1:16";
	pragma  "AMD RTI", "value:global_offset_2:u64:1:1:32";
	pragma  "AMD RTI", "pointer:printf_buffer:u8:1:1:48:uav:8:1:RW:0:0:0";
	pragma  "AMD RTI", "value:vqueue_pointer:u64:1:1:64";
	pragma  "AMD RTI", "value:aqlwrap_pointer:u64:1:1:80";
	pragma  "AMD RTI", "pointer:input:double:1:1:96:uav:8:8:RW:0:0:0";
	pragma  "AMD RTI", "value:length:u32:1:1:112";
	pragma  "AMD RTI", "value:stride:u32:1:1:128";
	pragma  "AMD RTI", "value:offset:u32:1:1:144";
	pragma  "AMD RTI", "pointer:result:u32:1:1:160:uav:8:4:RW:0:0:0";
	pragma  "AMD RTI", "function:1:0";
	pragma  "AMD RTI", "memory:64bitABI";
	pragma  "AMD RTI", "uavid:8";
	pragma  "AMD RTI", "privateid:8";
	pragma  "AMD RTI", "enqueue_kernel:0";
	pragma  "AMD RTI", "kernel_index:0";
	pragma  "AMD RTI", "reflection:0:size_t";
	pragma  "AMD RTI", "reflection:1:size_t";
	pragma  "AMD RTI", "reflection:2:size_t";
	pragma  "AMD RTI", "reflection:3:size_t";
	pragma  "AMD RTI", "reflection:4:size_t";
	pragma  "AMD RTI", "reflection:5:size_t";
	pragma  "AMD RTI", "reflection:6:double*";
	pragma  "AMD RTI", "reflection:7:int";
	pragma  "AMD RTI", "reflection:8:int";
	pragma  "AMD RTI", "reflection:9:int";
	pragma  "AMD RTI", "reflection:10:int*";
	pragma  "AMD RTI", "ARGEND:__OpenCL_run_kernel";
	group_f64 %__hsa_replaced_run_scratch[256];
	group_u32 %__hsa_replaced_run_scratch_index[256];

@__OpenCL_run_kernel_entry:
	// BB#0:                                // %entry
	workitemabsid_u32	$s0, 0;
	cvt_u64_u32	$d0, $s0;
	ld_kernarg_align(8)_width(all)_u64	$d1, [0];
	add_u64	$d2, $d0, $d1;
	ld_kernarg_align(4)_width(all)_u32	$s3, [%offset];
	cvt_u32_u64	$s0, $d2;
	ld_kernarg_align(4)_width(all)_u32	$s5, [%stride];
	ld_kernarg_align(4)_width(all)_u32	$s1, [%length];
	ld_kernarg_align(8)_width(all)_u64	$d1, [%input];
	cmp_ge_b1_s32	$c0, $s0, $s1;
	mov_b32	$s2, $s0;
	cbr_b1	$c0, @BB0_2;
	// BB#1:                                // %if.then
	mad_u32	$s2, $s0, $s5, $s3;
	cvt_s64_s32	$d0, $s2;
	shl_u64	$d0, $d0, 3;
	add_u64	$d0, $d1, $d0;
	ld_global_align(8)_f64	$d0, [$d0];
	gridsize_u32	$s2, 0;
	cvt_u64_u32	$d3, $s2;
	add_u64	$d2, $d3, $d2;
	cvt_u32_u64	$s2, $d2;

@BB0_2:
	// %for.cond.preheader
	cmp_ge_b1_s32	$c0, $s2, $s1;
	cbr_b1	$c0, @BB0_3;
	// BB#4:                                // %for.body.lr.ph
	mad_u32	$s3, $s2, $s5, $s3;
	gridsize_u32	$s4, 0;
	mul_u32	$s5, $s4, $s5;
	mov_b32	$s6, $s0;

@BB0_5:
	// %for.body
	cvt_s64_s32	$d2, $s3;
	shl_u64	$d2, $d2, 3;
	add_u64	$d2, $d1, $d2;
	ld_global_align(8)_f64	$d2, [$d2];
	cmp_gt_b1_f64	$c0, $d2, $d0;
	cmov_b32	$s6, $c0, $s6, $s2;
	add_u32	$s3, $s3, $s5;
	min_f64	$d0, $d2, $d0;
	add_u32	$s2, $s2, $s4;
	cmp_lt_b1_s32	$c0, $s2, $s1;
	cbr_b1	$c0, @BB0_5;
	br	@BB0_6;

@BB0_3:
	mov_b32	$s6, $s0;

@BB0_6:
	// %for.end
	workitemid_u32	$s3, 0;
	workgroupid_u32	$s2, 0;
	currentworkgroupsize_u32	$s4, 0;
	mul_u32	$s4, $s4, $s2;
	sub_u32	$s5, $s1, $s4;
	add_u32	$s7, $s3, 128;
	cmp_lt_b1_s32	$c0, $s7, $s5;
	cmp_lt_b1_s32	$c1, $s3, 128;
	and_b1	$c0, $c1, $c0;
	cvt_s64_s32	$d1, $s3;
	shl_u32	$s4, $s3, 3;
	st_group_align(8)_f64	$d0, [%__hsa_replaced_run_scratch][$s4];
	shl_u64	$d0, $d1, 2;
	st_group_align(4)_u32	$s6, [%__hsa_replaced_run_scratch_index][$d0];
	lda_group_u64	$d1, [%__hsa_replaced_run_scratch_index];
	add_u64	$d0, $d1, $d0;
	cmp_ne_b1_b1	$c0, $c0, 1;
	barrier;
	cbr_b1	$c0, @BB0_8;
	// BB#7:                                // %if.then41
	cvt_s64_s32	$d2, $s7;
	shl_u64	$d2, $d2, 2;
	add_u64	$d2, $d1, $d2;
	ld_group_align(8)_f64	$d3, [%__hsa_replaced_run_scratch][$s4];
	shl_u32	$s6, $s7, 3;
	ld_group_align(8)_f64	$d4, [%__hsa_replaced_run_scratch][$s6];
	cmp_gt_b1_f64	$c0, $d4, $d3;
	cmov_b64	$d2, $c0, $d0, $d2;
	min_f64	$d3, $d4, $d3;
	st_group_align(8)_f64	$d3, [%__hsa_replaced_run_scratch][$s4];
	ld_group_align(4)_u32	$s6, [$d2];
	shl_u32	$s7, $s3, 2;
	st_group_align(4)_u32	$s6, [%__hsa_replaced_run_scratch_index][$s7];

@BB0_8:
	// %if.end71
	add_u32	$s6, $s3, 64;
	cmp_lt_b1_s32	$c0, $s6, $s5;
	cmp_lt_b1_s32	$c1, $s3, 64;
	and_b1	$c0, $c1, $c0;
	cmp_ne_b1_b1	$c0, $c0, 1;
	barrier;
	cbr_b1	$c0, @BB0_10;
	// BB#9:                                // %if.then78
	cvt_s64_s32	$d2, $s6;
	shl_u64	$d2, $d2, 2;
	add_u64	$d2, $d1, $d2;
	ld_group_align(8)_f64	$d3, [%__hsa_replaced_run_scratch][$s4];
	shl_u32	$s6, $s6, 3;
	ld_group_align(8)_f64	$d4, [%__hsa_replaced_run_scratch][$s6];
	cmp_gt_b1_f64	$c0, $d4, $d3;
	cmov_b64	$d2, $c0, $d0, $d2;
	min_f64	$d3, $d4, $d3;
	st_group_align(8)_f64	$d3, [%__hsa_replaced_run_scratch][$s4];
	ld_group_align(4)_u32	$s6, [$d2];
	shl_u32	$s7, $s3, 2;
	st_group_align(4)_u32	$s6, [%__hsa_replaced_run_scratch_index][$s7];

@BB0_10:
	// %if.end110
	add_u32	$s6, $s3, 32;
	cmp_lt_b1_s32	$c0, $s6, $s5;
	cmp_lt_b1_s32	$c1, $s3, 32;
	and_b1	$c0, $c1, $c0;
	cmp_ne_b1_b1	$c0, $c0, 1;
	barrier;
	cbr_b1	$c0, @BB0_12;
	// BB#11:                                // %if.then117
	cvt_s64_s32	$d2, $s6;
	shl_u64	$d2, $d2, 2;
	add_u64	$d2, $d1, $d2;
	ld_group_align(8)_f64	$d3, [%__hsa_replaced_run_scratch][$s4];
	shl_u32	$s6, $s6, 3;
	ld_group_align(8)_f64	$d4, [%__hsa_replaced_run_scratch][$s6];
	cmp_gt_b1_f64	$c0, $d4, $d3;
	cmov_b64	$d2, $c0, $d0, $d2;
	min_f64	$d3, $d4, $d3;
	st_group_align(8)_f64	$d3, [%__hsa_replaced_run_scratch][$s4];
	ld_group_align(4)_u32	$s6, [$d2];
	shl_u32	$s7, $s3, 2;
	st_group_align(4)_u32	$s6, [%__hsa_replaced_run_scratch_index][$s7];

@BB0_12:
	// %if.end149
	add_u32	$s6, $s3, 16;
	cmp_lt_b1_s32	$c0, $s6, $s5;
	cmp_lt_b1_s32	$c1, $s3, 16;
	and_b1	$c0, $c1, $c0;
	cmp_ne_b1_b1	$c0, $c0, 1;
	barrier;
	cbr_b1	$c0, @BB0_14;
	// BB#13:                                // %if.then156
	cvt_s64_s32	$d2, $s6;
	shl_u64	$d2, $d2, 2;
	add_u64	$d2, $d1, $d2;
	ld_group_align(8)_f64	$d3, [%__hsa_replaced_run_scratch][$s4];
	shl_u32	$s6, $s6, 3;
	ld_group_align(8)_f64	$d4, [%__hsa_replaced_run_scratch][$s6];
	cmp_gt_b1_f64	$c0, $d4, $d3;
	cmov_b64	$d2, $c0, $d0, $d2;
	min_f64	$d3, $d4, $d3;
	st_group_align(8)_f64	$d3, [%__hsa_replaced_run_scratch][$s4];
	ld_group_align(4)_u32	$s6, [$d2];
	shl_u32	$s7, $s3, 2;
	st_group_align(4)_u32	$s6, [%__hsa_replaced_run_scratch_index][$s7];

@BB0_14:
	// %if.end188
	add_u32	$s6, $s3, 8;
	cmp_lt_b1_s32	$c0, $s6, $s5;
	cmp_lt_b1_s32	$c1, $s3, 8;
	and_b1	$c0, $c1, $c0;
	cmp_ne_b1_b1	$c0, $c0, 1;
	barrier;
	cbr_b1	$c0, @BB0_16;
	// BB#15:                                // %if.then195
	cvt_s64_s32	$d2, $s6;
	shl_u64	$d2, $d2, 2;
	add_u64	$d2, $d1, $d2;
	ld_group_align(8)_f64	$d3, [%__hsa_replaced_run_scratch][$s4];
	shl_u32	$s6, $s6, 3;
	ld_group_align(8)_f64	$d4, [%__hsa_replaced_run_scratch][$s6];
	cmp_gt_b1_f64	$c0, $d4, $d3;
	cmov_b64	$d2, $c0, $d0, $d2;
	min_f64	$d3, $d4, $d3;
	st_group_align(8)_f64	$d3, [%__hsa_replaced_run_scratch][$s4];
	ld_group_align(4)_u32	$s6, [$d2];
	shl_u32	$s7, $s3, 2;
	st_group_align(4)_u32	$s6, [%__hsa_replaced_run_scratch_index][$s7];

@BB0_16:
	// %if.end227
	add_u32	$s6, $s3, 4;
	cmp_lt_b1_s32	$c0, $s6, $s5;
	cmp_lt_b1_s32	$c1, $s3, 4;
	and_b1	$c0, $c1, $c0;
	cmp_ne_b1_b1	$c0, $c0, 1;
	barrier;
	cbr_b1	$c0, @BB0_18;
	// BB#17:                                // %if.then234
	cvt_s64_s32	$d2, $s6;
	shl_u64	$d2, $d2, 2;
	add_u64	$d2, $d1, $d2;
	ld_group_align(8)_f64	$d3, [%__hsa_replaced_run_scratch][$s4];
	shl_u32	$s6, $s6, 3;
	ld_group_align(8)_f64	$d4, [%__hsa_replaced_run_scratch][$s6];
	cmp_gt_b1_f64	$c0, $d4, $d3;
	cmov_b64	$d2, $c0, $d0, $d2;
	min_f64	$d3, $d4, $d3;
	st_group_align(8)_f64	$d3, [%__hsa_replaced_run_scratch][$s4];
	ld_group_align(4)_u32	$s6, [$d2];
	shl_u32	$s7, $s3, 2;
	st_group_align(4)_u32	$s6, [%__hsa_replaced_run_scratch_index][$s7];

@BB0_18:
	// %if.end266
	add_u32	$s6, $s3, 2;
	cmp_lt_b1_s32	$c0, $s6, $s5;
	cmp_lt_b1_s32	$c1, $s3, 2;
	and_b1	$c0, $c1, $c0;
	cmp_ne_b1_b1	$c0, $c0, 1;
	barrier;
	cbr_b1	$c0, @BB0_20;
	// BB#19:                                // %if.then273
	cvt_s64_s32	$d2, $s6;
	shl_u64	$d2, $d2, 2;
	add_u64	$d2, $d1, $d2;
	ld_group_align(8)_f64	$d3, [%__hsa_replaced_run_scratch][$s4];
	shl_u32	$s6, $s6, 3;
	ld_group_align(8)_f64	$d4, [%__hsa_replaced_run_scratch][$s6];
	cmp_gt_b1_f64	$c0, $d4, $d3;
	cmov_b64	$d2, $c0, $d0, $d2;
	min_f64	$d3, $d4, $d3;
	st_group_align(8)_f64	$d3, [%__hsa_replaced_run_scratch][$s4];
	ld_group_align(4)_u32	$s6, [$d2];
	shl_u32	$s7, $s3, 2;
	st_group_align(4)_u32	$s6, [%__hsa_replaced_run_scratch_index][$s7];

@BB0_20:
	// %if.end305
	add_u32	$s6, $s3, 1;
	cmp_lt_b1_s32	$c0, $s6, $s5;
	cmp_lt_b1_s32	$c1, $s3, 1;
	and_b1	$c0, $c1, $c0;
	cmp_ne_b1_b1	$c0, $c0, 1;
	barrier;
	cbr_b1	$c0, @BB0_22;
	// BB#21:                                // %if.then312
	cvt_s64_s32	$d2, $s6;
	shl_u64	$d2, $d2, 2;
	add_u64	$d1, $d1, $d2;
	ld_group_align(8)_f64	$d2, [%__hsa_replaced_run_scratch][$s4];
	shl_u32	$s5, $s6, 3;
	ld_group_align(8)_f64	$d3, [%__hsa_replaced_run_scratch][$s5];
	cmp_gt_b1_f64	$c0, $d3, $d2;
	cmov_b64	$d0, $c0, $d0, $d1;
	min_f64	$d1, $d3, $d2;
	st_group_align(8)_f64	$d1, [%__hsa_replaced_run_scratch][$s4];
	ld_group_align(4)_u32	$s4, [$d0];
	shl_u32	$s5, $s3, 2;
	st_group_align(4)_u32	$s4, [%__hsa_replaced_run_scratch_index][$s5];

@BB0_22:
	// %if.end344
	cmp_lt_b1_s32	$c0, $s0, $s1;
	cmp_eq_b1_s32	$c1, $s3, 0;
	and_b1	$c0, $c0, $c1;
	cmp_ne_b1_b1	$c0, $c0, 1;
	barrier;
	cbr_b1	$c0, @BB0_24;
	// BB#23:                                // %if.then351
	cvt_u64_u32	$d0, $s2;
	ld_kernarg_align(8)_width(all)_u64	$d1, [%result];
	shl_u64	$d0, $d0, 2;
	add_u64	$d0, $d1, $d0;
	ld_group_align(4)_width(WAVESIZE)_u32	$s0, [%__hsa_replaced_run_scratch_index];
	st_global_align(4)_u32	$s0, [$d0];

@BB0_24:
	// %return
	ret;
};
