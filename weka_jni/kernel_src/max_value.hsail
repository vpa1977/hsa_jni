version 0:20140528:$full:$large;
extension "amd:gcn";
extension "IMAGE";

decl prog function &abort()();

prog kernel &__OpenCL_run_kernel(
	kernarg_u64 %global_offset_0,
	kernarg_u64 %global_offset_1,
	kernarg_u64 %global_offset_2,
	kernarg_u64 %printf_buffer,
	kernarg_u64 %vqueue_pointer,
	kernarg_u64 %aqlwrap_pointer,
	kernarg_u64 %input,
	kernarg_u32 %length,
	kernarg_u64 %result)
{
	pragma  "AMD RTI", "ARGSTART:__OpenCL_run_kernel";
	pragma  "AMD RTI", "version:3:1:104";
	pragma  "AMD RTI", "device:generic";
	pragma  "AMD RTI", "uniqueid:1034";
	pragma  "AMD RTI", "memory:private:16";
	pragma  "AMD RTI", "memory:region:0";
	pragma  "AMD RTI", "memory:local:3072";
	pragma  "AMD RTI", "printf_fmt:1:2:8:4:Hola %f  and %d\n";
	pragma  "AMD RTI", "value:global_offset_0:u64:1:1:0";
	pragma  "AMD RTI", "value:global_offset_1:u64:1:1:16";
	pragma  "AMD RTI", "value:global_offset_2:u64:1:1:32";
	pragma  "AMD RTI", "pointer:printf_buffer:u8:1:1:48:uav:8:1:RW:0:0:0";
	pragma  "AMD RTI", "value:vqueue_pointer:u64:1:1:64";
	pragma  "AMD RTI", "value:aqlwrap_pointer:u64:1:1:80";
	pragma  "AMD RTI", "pointer:input:double:1:1:96:uav:8:8:RW:0:0:0";
	pragma  "AMD RTI", "value:length:u32:1:1:112";
	pragma  "AMD RTI", "pointer:result:u32:1:1:128:uav:8:4:RW:0:0:0";
	pragma  "AMD RTI", "function:1:0";
	pragma  "AMD RTI", "memory:64bitABI";
	pragma  "AMD RTI", "uavid:8";
	pragma  "AMD RTI", "privateid:8";
	pragma  "AMD RTI", "enqueue_kernel:0";
	pragma  "AMD RTI", "kernel_index:0";
	pragma  "AMD RTI", "reflection:0:size_t";
	pragma  "AMD RTI", "reflection:1:size_t";
	pragma  "AMD RTI", "reflection:2:size_t";
	pragma  "AMD RTI", "reflection:3:size_t";
	pragma  "AMD RTI", "reflection:4:size_t";
	pragma  "AMD RTI", "reflection:5:size_t";
	pragma  "AMD RTI", "reflection:6:double*";
	pragma  "AMD RTI", "reflection:7:int";
	pragma  "AMD RTI", "reflection:8:int*";
	pragma  "AMD RTI", "ARGEND:__OpenCL_run_kernel";
	group_f64 %__hsa_replaced_run_scratch[256];
	group_u32 %__hsa_replaced_run_scratch_index[256];
	align(4) private_u8 %privateStack[4];

@__OpenCL_run_kernel_entry:
	// BB#0:                                // %entry
	workitemabsid_u32	$s0, 0;
	cvt_u64_u32	$d0, $s0;
	ld_kernarg_align(8)_width(all)_u64	$d1, [0];
	add_u64	$d0, $d0, $d1;
	cvt_u32_u64	$s3, $d0;
	ld_kernarg_align(8)_width(all)_u64	$d0, [%result];
	ld_kernarg_align(4)_width(all)_u32	$s0, [%length];
	ld_kernarg_align(8)_width(all)_u64	$d1, [%input];
	cmp_ge_b1_s32	$c0, $s3, $s0;
	mov_b32	$s1, $s3;
	cbr_b1	$c0, @BB0_2;
	// BB#1:                                // %if.then
	gridsize_u32	$s1, 0;
	add_u32	$s1, $s1, $s3;
	cvt_s64_s32	$d2, $s3;
	shl_u64	$d2, $d2, 3;
	add_u64	$d2, $d1, $d2;
	ld_global_align(8)_f64	$d3, [$d2];

@BB0_2:
	// %for.cond.preheader
	cmp_ge_b1_s32	$c1, $s1, $s0;
	cbr_b1	$c1, @BB0_5;
	// BB#3:                                // %for.body.lr.ph
	gridsize_u32	$s2, 0;

@BB0_4:
	// %for.body
	cvt_s64_s32	$d2, $s1;
	shl_u64	$d2, $d2, 3;
	add_u64	$d2, $d1, $d2;
	ld_global_align(8)_f64	$d2, [$d2];
	cmp_lt_b1_f64	$c1, $d2, $d3;
	cmov_b32	$s3, $c1, $s3, $s1;
	max_f64	$d3, $d2, $d3;
	add_u32	$s1, $s1, $s2;
	cmp_lt_b1_s32	$c1, $s1, $s0;
	cbr_b1	$c1, @BB0_4;

@BB0_5:
	// %for.end
	workitemid_u32	$s1, 0;
	cvt_s64_s32	$d1, $s1;
	shl_u32	$s2, $s1, 3;
	st_group_align(8)_f64	$d3, [%__hsa_replaced_run_scratch][$s2];
	shl_u64	$d1, $d1, 2;
	st_group_align(4)_u32	$s3, [%__hsa_replaced_run_scratch_index][$d1];
	barrier;
	ld_kernarg_align(8)_width(all)_u64	$d4, [24];
	lda_group_u64	$d2, [%__hsa_replaced_run_scratch_index];
	ld_global_align(4)_width(all)_u32	$s6, [$d4+4];
	atomic_ld_global_scacq_wg_equiv(255)_b32	$s7, [$d4];
	st_private_align(4)_u32	$s7, [%privateStack];
	add_u64	$d1, $d2, $d1;
	mov_b32	$s4, 0;
	mov_b32	$s8, 1;

@BB0_6:
	add_u32	$s5, $s7, 24;
	cmp_gt_b1_u32	$c1, $s5, $s6;
	cbr_b1	$c1, @BB0_9;
	// BB#7:
	add_u32	$s7, $s7, 16;
	ld_private_align(4)_u32	$s5, [%privateStack];
	atomic_cas_global_scar_wg_equiv(255)_b32	$s7, [$d4], $s5, $s7;
	st_private_align(4)_u32	$s7, [%privateStack];
	cmp_ne_b1_s32	$c1, $s7, $s5;
	cbr_b1	$c1, @BB0_6;
	// BB#8:
	mov_b32	$s4, $s8;

@BB0_9:
	// %unified_loop_exit
	cmp_eq_b1_s32	$c1, $s4, 0;
	cbr_b1	$c1, @BB0_13;
	// BB#10:                                // %unified_loop_exit
	cmp_ne_b1_s32	$c1, $s4, 1;
	cbr_b1	$c1, @BB0_13;
	// BB#11:                                // %__printf_alloc.exit
	cvt_u64_u32	$d5, $s5;
	add_u64	$d4, $d5, $d4;
	add_u64	$d5, $d4, 8;
	nullptr_global_u64	$d6;
	cmp_eq_b1_s64	$c1, $d5, $d6;
	cbr_b1	$c1, @BB0_13;
	// BB#12:
	mov_b32	$s4, 1;
	st_global_align(4)_u32	$s4, [$d5];
	st_global_align(8)_f64	$d3, [$d4+12];
	st_global_align(4)_u32	$s3, [$d4+20];

@BB0_13:
	// %__printf_alloc.exit.thread
	workgroupid_u32	$s3, 0;
	currentworkgroupsize_u32	$s4, 0;
	mul_u32	$s4, $s4, $s3;
	sub_u32	$s4, $s0, $s4;
	add_u32	$s5, $s1, 128;
	cmp_lt_b1_s32	$c1, $s5, $s4;
	cmp_lt_b1_s32	$c2, $s1, 128;
	and_b1	$c1, $c2, $c1;
	cmp_ne_b1_b1	$c1, $c1, 1;
	cbr_b1	$c1, @BB0_15;
	// BB#14:                                // %if.then42
	cvt_s64_s32	$d3, $s5;
	shl_u64	$d3, $d3, 2;
	add_u64	$d3, $d2, $d3;
	ld_group_align(8)_f64	$d4, [%__hsa_replaced_run_scratch][$s2];
	shl_u32	$s5, $s5, 3;
	ld_group_align(8)_f64	$d5, [%__hsa_replaced_run_scratch][$s5];
	cmp_lt_b1_f64	$c1, $d5, $d4;
	cmov_b64	$d3, $c1, $d1, $d3;
	max_f64	$d4, $d5, $d4;
	st_group_align(8)_f64	$d4, [%__hsa_replaced_run_scratch][$s2];
	ld_group_align(4)_u32	$s5, [$d3];
	shl_u32	$s6, $s1, 2;
	st_group_align(4)_u32	$s5, [%__hsa_replaced_run_scratch_index][$s6];

@BB0_15:
	// %if.end72
	add_u32	$s5, $s1, 64;
	cmp_lt_b1_s32	$c1, $s5, $s4;
	cmp_lt_b1_s32	$c2, $s1, 64;
	and_b1	$c1, $c2, $c1;
	cmp_ne_b1_b1	$c1, $c1, 1;
	barrier;
	cbr_b1	$c1, @BB0_17;
	// BB#16:                                // %if.then79
	cvt_s64_s32	$d3, $s5;
	shl_u64	$d3, $d3, 2;
	add_u64	$d3, $d2, $d3;
	ld_group_align(8)_f64	$d4, [%__hsa_replaced_run_scratch][$s2];
	shl_u32	$s5, $s5, 3;
	ld_group_align(8)_f64	$d5, [%__hsa_replaced_run_scratch][$s5];
	cmp_lt_b1_f64	$c1, $d5, $d4;
	cmov_b64	$d3, $c1, $d1, $d3;
	max_f64	$d4, $d5, $d4;
	st_group_align(8)_f64	$d4, [%__hsa_replaced_run_scratch][$s2];
	ld_group_align(4)_u32	$s5, [$d3];
	shl_u32	$s6, $s1, 2;
	st_group_align(4)_u32	$s5, [%__hsa_replaced_run_scratch_index][$s6];

@BB0_17:
	// %if.end111
	add_u32	$s5, $s1, 32;
	cmp_lt_b1_s32	$c1, $s5, $s4;
	cmp_lt_b1_s32	$c2, $s1, 32;
	and_b1	$c1, $c2, $c1;
	cmp_ne_b1_b1	$c1, $c1, 1;
	barrier;
	cbr_b1	$c1, @BB0_19;
	// BB#18:                                // %if.then118
	cvt_s64_s32	$d3, $s5;
	shl_u64	$d3, $d3, 2;
	add_u64	$d3, $d2, $d3;
	ld_group_align(8)_f64	$d4, [%__hsa_replaced_run_scratch][$s2];
	shl_u32	$s5, $s5, 3;
	ld_group_align(8)_f64	$d5, [%__hsa_replaced_run_scratch][$s5];
	cmp_lt_b1_f64	$c1, $d5, $d4;
	cmov_b64	$d3, $c1, $d1, $d3;
	max_f64	$d4, $d5, $d4;
	st_group_align(8)_f64	$d4, [%__hsa_replaced_run_scratch][$s2];
	ld_group_align(4)_u32	$s5, [$d3];
	shl_u32	$s6, $s1, 2;
	st_group_align(4)_u32	$s5, [%__hsa_replaced_run_scratch_index][$s6];

@BB0_19:
	// %if.end150
	add_u32	$s5, $s1, 16;
	cmp_lt_b1_s32	$c1, $s5, $s4;
	cmp_lt_b1_s32	$c2, $s1, 16;
	and_b1	$c1, $c2, $c1;
	cmp_ne_b1_b1	$c1, $c1, 1;
	barrier;
	cbr_b1	$c1, @BB0_21;
	// BB#20:                                // %if.then157
	cvt_s64_s32	$d3, $s5;
	shl_u64	$d3, $d3, 2;
	add_u64	$d3, $d2, $d3;
	ld_group_align(8)_f64	$d4, [%__hsa_replaced_run_scratch][$s2];
	shl_u32	$s5, $s5, 3;
	ld_group_align(8)_f64	$d5, [%__hsa_replaced_run_scratch][$s5];
	cmp_lt_b1_f64	$c1, $d5, $d4;
	cmov_b64	$d3, $c1, $d1, $d3;
	max_f64	$d4, $d5, $d4;
	st_group_align(8)_f64	$d4, [%__hsa_replaced_run_scratch][$s2];
	ld_group_align(4)_u32	$s5, [$d3];
	shl_u32	$s6, $s1, 2;
	st_group_align(4)_u32	$s5, [%__hsa_replaced_run_scratch_index][$s6];

@BB0_21:
	// %if.end189
	add_u32	$s5, $s1, 8;
	cmp_lt_b1_s32	$c1, $s5, $s4;
	cmp_lt_b1_s32	$c2, $s1, 8;
	and_b1	$c1, $c2, $c1;
	cmp_ne_b1_b1	$c1, $c1, 1;
	barrier;
	cbr_b1	$c1, @BB0_23;
	// BB#22:                                // %if.then196
	cvt_s64_s32	$d3, $s5;
	shl_u64	$d3, $d3, 2;
	add_u64	$d3, $d2, $d3;
	ld_group_align(8)_f64	$d4, [%__hsa_replaced_run_scratch][$s2];
	shl_u32	$s5, $s5, 3;
	ld_group_align(8)_f64	$d5, [%__hsa_replaced_run_scratch][$s5];
	cmp_lt_b1_f64	$c1, $d5, $d4;
	cmov_b64	$d3, $c1, $d1, $d3;
	max_f64	$d4, $d5, $d4;
	st_group_align(8)_f64	$d4, [%__hsa_replaced_run_scratch][$s2];
	ld_group_align(4)_u32	$s5, [$d3];
	shl_u32	$s6, $s1, 2;
	st_group_align(4)_u32	$s5, [%__hsa_replaced_run_scratch_index][$s6];

@BB0_23:
	// %if.end228
	add_u32	$s5, $s1, 4;
	cmp_lt_b1_s32	$c1, $s5, $s4;
	cmp_lt_b1_s32	$c2, $s1, 4;
	and_b1	$c1, $c2, $c1;
	cmp_ne_b1_b1	$c1, $c1, 1;
	barrier;
	cbr_b1	$c1, @BB0_25;
	// BB#24:                                // %if.then235
	cvt_s64_s32	$d3, $s5;
	shl_u64	$d3, $d3, 2;
	add_u64	$d3, $d2, $d3;
	ld_group_align(8)_f64	$d4, [%__hsa_replaced_run_scratch][$s2];
	shl_u32	$s5, $s5, 3;
	ld_group_align(8)_f64	$d5, [%__hsa_replaced_run_scratch][$s5];
	cmp_lt_b1_f64	$c1, $d5, $d4;
	cmov_b64	$d3, $c1, $d1, $d3;
	max_f64	$d4, $d5, $d4;
	st_group_align(8)_f64	$d4, [%__hsa_replaced_run_scratch][$s2];
	ld_group_align(4)_u32	$s5, [$d3];
	shl_u32	$s6, $s1, 2;
	st_group_align(4)_u32	$s5, [%__hsa_replaced_run_scratch_index][$s6];

@BB0_25:
	// %if.end267
	add_u32	$s5, $s1, 2;
	cmp_lt_b1_s32	$c1, $s5, $s4;
	cmp_lt_b1_s32	$c2, $s1, 2;
	and_b1	$c1, $c2, $c1;
	cmp_ne_b1_b1	$c1, $c1, 1;
	barrier;
	cbr_b1	$c1, @BB0_27;
	// BB#26:                                // %if.then274
	cvt_s64_s32	$d3, $s5;
	shl_u64	$d3, $d3, 2;
	add_u64	$d3, $d2, $d3;
	ld_group_align(8)_f64	$d4, [%__hsa_replaced_run_scratch][$s2];
	shl_u32	$s5, $s5, 3;
	ld_group_align(8)_f64	$d5, [%__hsa_replaced_run_scratch][$s5];
	cmp_lt_b1_f64	$c1, $d5, $d4;
	cmov_b64	$d3, $c1, $d1, $d3;
	max_f64	$d4, $d5, $d4;
	st_group_align(8)_f64	$d4, [%__hsa_replaced_run_scratch][$s2];
	ld_group_align(4)_u32	$s5, [$d3];
	shl_u32	$s6, $s1, 2;
	st_group_align(4)_u32	$s5, [%__hsa_replaced_run_scratch_index][$s6];

@BB0_27:
	// %if.end306
	add_u32	$s5, $s1, 1;
	cmp_lt_b1_s32	$c1, $s5, $s4;
	cmp_lt_b1_s32	$c2, $s1, 1;
	and_b1	$c1, $c2, $c1;
	cmp_ne_b1_b1	$c1, $c1, 1;
	barrier;
	cbr_b1	$c1, @BB0_29;
	// BB#28:                                // %if.then313
	cvt_s64_s32	$d3, $s5;
	shl_u64	$d3, $d3, 2;
	add_u64	$d2, $d2, $d3;
	ld_group_align(8)_f64	$d3, [%__hsa_replaced_run_scratch][$s2];
	shl_u32	$s4, $s5, 3;
	ld_group_align(8)_f64	$d4, [%__hsa_replaced_run_scratch][$s4];
	cmp_lt_b1_f64	$c1, $d4, $d3;
	cmov_b64	$d1, $c1, $d1, $d2;
	max_f64	$d2, $d4, $d3;
	st_group_align(8)_f64	$d2, [%__hsa_replaced_run_scratch][$s2];
	ld_group_align(4)_u32	$s2, [$d1];
	shl_u32	$s4, $s1, 2;
	st_group_align(4)_u32	$s2, [%__hsa_replaced_run_scratch_index][$s4];

@BB0_29:
	// %if.end345
	barrier;
	cbr_b1	$c0, @BB0_33;
	// BB#30:                                // %if.end349
	cmp_ne_b1_s32	$c0, $s1, 0;
	cbr_b1	$c0, @BB0_32;
	// BB#31:                                // %if.then352
	cvt_u64_u32	$d1, $s3;
	shl_u64	$d1, $d1, 2;
	add_u64	$d1, $d0, $d1;
	ld_group_align(4)_width(WAVESIZE)_u32	$s1, [%__hsa_replaced_run_scratch_index];
	st_global_align(4)_u32	$s1, [$d1];

@BB0_32:
	// %if.end355
	st_global_align(4)_u32	$s0, [$d0+132];

@BB0_33:
	// %return
	ret;
};
