version 0:20140528:$full:$large;
extension "amd:gcn";
extension "IMAGE";

decl prog function &_Z6selectiil(arg_u32 %ret)(
	arg_u32 %arg_p0,
	arg_u32 %arg_p1,
	arg_u64 %arg_p2);

decl prog function &abort()();

prog kernel &__OpenCL_run_kernel(
	kernarg_u64 %global_offset_0,
	kernarg_u64 %global_offset_1,
	kernarg_u64 %global_offset_2,
	kernarg_u64 %printf_buffer,
	kernarg_u64 %vqueue_pointer,
	kernarg_u64 %aqlwrap_pointer,
	kernarg_u64 %input,
	kernarg_u32 %length,
	kernarg_u32 %stride,
	kernarg_u32 %offset,
	kernarg_u64 %result)
{
	pragma  "AMD RTI", "ARGSTART:__OpenCL_run_kernel";
	pragma  "AMD RTI", "version:3:1:104";
	pragma  "AMD RTI", "device:generic";
	pragma  "AMD RTI", "uniqueid:1033";
	pragma  "AMD RTI", "memory:private:0";
	pragma  "AMD RTI", "memory:region:0";
	pragma  "AMD RTI", "memory:local:3072";
	pragma  "AMD RTI", "value:global_offset_0:u64:1:1:0";
	pragma  "AMD RTI", "value:global_offset_1:u64:1:1:16";
	pragma  "AMD RTI", "value:global_offset_2:u64:1:1:32";
	pragma  "AMD RTI", "pointer:printf_buffer:u8:1:1:48:uav:8:1:RW:0:0:0";
	pragma  "AMD RTI", "value:vqueue_pointer:u64:1:1:64";
	pragma  "AMD RTI", "value:aqlwrap_pointer:u64:1:1:80";
	pragma  "AMD RTI", "pointer:input:double:1:1:96:uav:8:8:RW:0:0:0";
	pragma  "AMD RTI", "value:length:u32:1:1:112";
	pragma  "AMD RTI", "value:stride:u32:1:1:128";
	pragma  "AMD RTI", "value:offset:u32:1:1:144";
	pragma  "AMD RTI", "pointer:result:u32:1:1:160:uav:8:4:RW:0:0:0";
	pragma  "AMD RTI", "function:1:0";
	pragma  "AMD RTI", "memory:64bitABI";
	pragma  "AMD RTI", "uavid:8";
	pragma  "AMD RTI", "privateid:8";
	pragma  "AMD RTI", "enqueue_kernel:0";
	pragma  "AMD RTI", "kernel_index:0";
	pragma  "AMD RTI", "reflection:0:size_t";
	pragma  "AMD RTI", "reflection:1:size_t";
	pragma  "AMD RTI", "reflection:2:size_t";
	pragma  "AMD RTI", "reflection:3:size_t";
	pragma  "AMD RTI", "reflection:4:size_t";
	pragma  "AMD RTI", "reflection:5:size_t";
	pragma  "AMD RTI", "reflection:6:double*";
	pragma  "AMD RTI", "reflection:7:int";
	pragma  "AMD RTI", "reflection:8:int";
	pragma  "AMD RTI", "reflection:9:int";
	pragma  "AMD RTI", "reflection:10:int*";
	pragma  "AMD RTI", "ARGEND:__OpenCL_run_kernel";
	group_f64 %__hsa_replaced_run_scratch[256];
	group_u32 %__hsa_replaced_run_scratch_index[256];

@__OpenCL_run_kernel_entry:
	// BB#0:                                // %entry
	workitemabsid_u32	$s0, 0;
	cvt_u64_u32	$d0, $s0;
	ld_kernarg_align(8)_width(all)_u64	$d1, [0];
	add_u64	$d2, $d0, $d1;
	cvt_u32_u64	$s0, $d2;
	ld_kernarg_align(4)_width(all)_u32	$s1, [%offset];
	ld_kernarg_align(4)_width(all)_u32	$s4, [%stride];
	mad_u32	$s5, $s0, $s4, $s1;
	ld_kernarg_align(4)_width(all)_u32	$s2, [%length];
	ld_kernarg_align(8)_width(all)_u64	$d1, [%input];
	cmp_ge_b1_s32	$c0, $s0, $s2;
	cbr_b1	$c0, @BB0_2;
	// BB#1:                                // %if.then
	gridsize_u32	$s0, 0;
	cvt_u64_u32	$d0, $s0;
	add_u64	$d0, $d0, $d2;
	cvt_u32_u64	$s0, $d0;
	cvt_s64_s32	$d0, $s5;
	shl_u64	$d0, $d0, 3;
	add_u64	$d0, $d1, $d0;
	ld_global_align(8)_f64	$d0, [$d0];

@BB0_2:
	// %for.cond.preheader
	cmp_ge_b1_s32	$c0, $s0, $s2;
	cbr_b1	$c0, @BB0_5;
	// BB#3:                                // %for.body.lr.ph
	mad_u32	$s1, $s0, $s4, $s1;
	gridsize_u32	$s3, 0;
	mul_u32	$s4, $s3, $s4;

@BB0_4:
	// %for.body
	cvt_s64_s32	$d2, $s1;
	shl_u64	$d2, $d2, 3;
	add_u64	$d2, $d1, $d2;
	ld_global_align(8)_f64	$d2, [$d2];
	cmp_lt_b1_f64	$c0, $d2, $d0;
	cmov_b32	$s5, $c0, $s5, $s1;
	add_u32	$s1, $s1, $s4;
	max_f64	$d0, $d2, $d0;
	add_u32	$s0, $s0, $s3;
	barrier;
	cmp_lt_b1_s32	$c0, $s0, $s2;
	cbr_b1	$c0, @BB0_4;

@BB0_5:
	// %for.end
	workitemid_u32	$s1, 0;
	workgroupid_u32	$s0, 0;
	currentworkgroupsize_u32	$s3, 0;
	mul_u32	$s3, $s3, $s0;
	sub_u32	$s4, $s2, $s3;
	add_u32	$s6, $s1, 128;
	cmp_lt_b1_s32	$c0, $s6, $s4;
	cmp_lt_b1_s32	$c1, $s1, 128;
	and_b1	$c0, $c1, $c0;
	shl_u32	$s3, $s1, 3;
	st_group_align(8)_f64	$d0, [%__hsa_replaced_run_scratch][$s3];
	shl_u32	$s2, $s1, 2;
	st_group_align(4)_u32	$s5, [%__hsa_replaced_run_scratch_index][$s2];
	cmp_ne_b1_b1	$c0, $c0, 1;
	barrier;
	cbr_b1	$c0, @BB0_7;
	// BB#6:                                // %if.then43
	ld_group_align(8)_f64	$d1, [%__hsa_replaced_run_scratch][$s3];
	shl_u32	$s5, $s6, 3;
	ld_group_align(8)_f64	$d2, [%__hsa_replaced_run_scratch][$s5];
	cmp_lt_b1_f64	$c0, $d2, $d1;
	cvt_u64_b1	$d0, $c0;
	max_f64	$d1, $d2, $d1;
	st_group_align(8)_f64	$d1, [%__hsa_replaced_run_scratch][$s3];
	ld_group_align(4)_u32	$s5, [%__hsa_replaced_run_scratch_index][$s2];
	shl_u32	$s6, $s6, 2;
	ld_group_align(4)_u32	$s6, [%__hsa_replaced_run_scratch_index][$s6];
	{
		arg_u32 %_Z6selectiil;
		arg_u32 %__param_p0;
		arg_u32 %__param_p1;
		arg_u64 %__param_p2;
		st_arg_align(4)_u32	$s6, [%__param_p0];
		st_arg_align(4)_u32	$s5, [%__param_p1];
		st_arg_align(8)_u64	$d0, [%__param_p2];
		call	&_Z6selectiil (%_Z6selectiil) (%__param_p0, %__param_p1, %__param_p2);
		ld_arg_align(4)_u32	$s5, [%_Z6selectiil];
	}
	st_group_align(4)_u32	$s5, [%__hsa_replaced_run_scratch_index][$s2];

@BB0_7:
	// %if.end62
	add_u32	$s5, $s1, 64;
	cmp_lt_b1_s32	$c0, $s5, $s4;
	cmp_lt_b1_s32	$c1, $s1, 64;
	and_b1	$c0, $c1, $c0;
	cmp_ne_b1_b1	$c0, $c0, 1;
	barrier;
	cbr_b1	$c0, @BB0_9;
	// BB#8:                                // %if.then69
	ld_group_align(8)_f64	$d1, [%__hsa_replaced_run_scratch][$s3];
	shl_u32	$s6, $s5, 3;
	ld_group_align(8)_f64	$d2, [%__hsa_replaced_run_scratch][$s6];
	cmp_lt_b1_f64	$c0, $d2, $d1;
	cvt_u64_b1	$d0, $c0;
	max_f64	$d1, $d2, $d1;
	st_group_align(8)_f64	$d1, [%__hsa_replaced_run_scratch][$s3];
	ld_group_align(4)_u32	$s6, [%__hsa_replaced_run_scratch_index][$s2];
	shl_u32	$s5, $s5, 2;
	ld_group_align(4)_u32	$s5, [%__hsa_replaced_run_scratch_index][$s5];
	{
		arg_u32 %_Z6selectiil;
		arg_u32 %__param_p0;
		arg_u32 %__param_p1;
		arg_u64 %__param_p2;
		st_arg_align(4)_u32	$s5, [%__param_p0];
		st_arg_align(4)_u32	$s6, [%__param_p1];
		st_arg_align(8)_u64	$d0, [%__param_p2];
		call	&_Z6selectiil (%_Z6selectiil) (%__param_p0, %__param_p1, %__param_p2);
		ld_arg_align(4)_u32	$s5, [%_Z6selectiil];
	}
	st_group_align(4)_u32	$s5, [%__hsa_replaced_run_scratch_index][$s2];

@BB0_9:
	// %if.end90
	add_u32	$s5, $s1, 32;
	cmp_lt_b1_s32	$c0, $s5, $s4;
	cmp_lt_b1_s32	$c1, $s1, 32;
	and_b1	$c0, $c1, $c0;
	cmp_ne_b1_b1	$c0, $c0, 1;
	barrier;
	cbr_b1	$c0, @BB0_11;
	// BB#10:                                // %if.then97
	ld_group_align(8)_f64	$d1, [%__hsa_replaced_run_scratch][$s3];
	shl_u32	$s6, $s5, 3;
	ld_group_align(8)_f64	$d2, [%__hsa_replaced_run_scratch][$s6];
	cmp_lt_b1_f64	$c0, $d2, $d1;
	cvt_u64_b1	$d0, $c0;
	max_f64	$d1, $d2, $d1;
	st_group_align(8)_f64	$d1, [%__hsa_replaced_run_scratch][$s3];
	ld_group_align(4)_u32	$s6, [%__hsa_replaced_run_scratch_index][$s2];
	shl_u32	$s5, $s5, 2;
	ld_group_align(4)_u32	$s5, [%__hsa_replaced_run_scratch_index][$s5];
	{
		arg_u32 %_Z6selectiil;
		arg_u32 %__param_p0;
		arg_u32 %__param_p1;
		arg_u64 %__param_p2;
		st_arg_align(4)_u32	$s5, [%__param_p0];
		st_arg_align(4)_u32	$s6, [%__param_p1];
		st_arg_align(8)_u64	$d0, [%__param_p2];
		call	&_Z6selectiil (%_Z6selectiil) (%__param_p0, %__param_p1, %__param_p2);
		ld_arg_align(4)_u32	$s5, [%_Z6selectiil];
	}
	st_group_align(4)_u32	$s5, [%__hsa_replaced_run_scratch_index][$s2];

@BB0_11:
	// %if.end118
	add_u32	$s5, $s1, 16;
	cmp_lt_b1_s32	$c0, $s5, $s4;
	cmp_lt_b1_s32	$c1, $s1, 16;
	and_b1	$c0, $c1, $c0;
	cmp_ne_b1_b1	$c0, $c0, 1;
	barrier;
	cbr_b1	$c0, @BB0_13;
	// BB#12:                                // %if.then125
	ld_group_align(8)_f64	$d1, [%__hsa_replaced_run_scratch][$s3];
	shl_u32	$s6, $s5, 3;
	ld_group_align(8)_f64	$d2, [%__hsa_replaced_run_scratch][$s6];
	cmp_lt_b1_f64	$c0, $d2, $d1;
	cvt_u64_b1	$d0, $c0;
	max_f64	$d1, $d2, $d1;
	st_group_align(8)_f64	$d1, [%__hsa_replaced_run_scratch][$s3];
	ld_group_align(4)_u32	$s6, [%__hsa_replaced_run_scratch_index][$s2];
	shl_u32	$s5, $s5, 2;
	ld_group_align(4)_u32	$s5, [%__hsa_replaced_run_scratch_index][$s5];
	{
		arg_u32 %_Z6selectiil;
		arg_u32 %__param_p0;
		arg_u32 %__param_p1;
		arg_u64 %__param_p2;
		st_arg_align(4)_u32	$s5, [%__param_p0];
		st_arg_align(4)_u32	$s6, [%__param_p1];
		st_arg_align(8)_u64	$d0, [%__param_p2];
		call	&_Z6selectiil (%_Z6selectiil) (%__param_p0, %__param_p1, %__param_p2);
		ld_arg_align(4)_u32	$s5, [%_Z6selectiil];
	}
	st_group_align(4)_u32	$s5, [%__hsa_replaced_run_scratch_index][$s2];

@BB0_13:
	// %if.end146
	add_u32	$s5, $s1, 8;
	cmp_lt_b1_s32	$c0, $s5, $s4;
	cmp_lt_b1_s32	$c1, $s1, 8;
	and_b1	$c0, $c1, $c0;
	cmp_ne_b1_b1	$c0, $c0, 1;
	barrier;
	cbr_b1	$c0, @BB0_15;
	// BB#14:                                // %if.then153
	ld_group_align(8)_f64	$d1, [%__hsa_replaced_run_scratch][$s3];
	shl_u32	$s6, $s5, 3;
	ld_group_align(8)_f64	$d2, [%__hsa_replaced_run_scratch][$s6];
	cmp_lt_b1_f64	$c0, $d2, $d1;
	cvt_u64_b1	$d0, $c0;
	max_f64	$d1, $d2, $d1;
	st_group_align(8)_f64	$d1, [%__hsa_replaced_run_scratch][$s3];
	ld_group_align(4)_u32	$s6, [%__hsa_replaced_run_scratch_index][$s2];
	shl_u32	$s5, $s5, 2;
	ld_group_align(4)_u32	$s5, [%__hsa_replaced_run_scratch_index][$s5];
	{
		arg_u32 %_Z6selectiil;
		arg_u32 %__param_p0;
		arg_u32 %__param_p1;
		arg_u64 %__param_p2;
		st_arg_align(4)_u32	$s5, [%__param_p0];
		st_arg_align(4)_u32	$s6, [%__param_p1];
		st_arg_align(8)_u64	$d0, [%__param_p2];
		call	&_Z6selectiil (%_Z6selectiil) (%__param_p0, %__param_p1, %__param_p2);
		ld_arg_align(4)_u32	$s5, [%_Z6selectiil];
	}
	st_group_align(4)_u32	$s5, [%__hsa_replaced_run_scratch_index][$s2];

@BB0_15:
	// %if.end174
	add_u32	$s5, $s1, 4;
	cmp_lt_b1_s32	$c0, $s5, $s4;
	cmp_lt_b1_s32	$c1, $s1, 4;
	and_b1	$c0, $c1, $c0;
	cmp_ne_b1_b1	$c0, $c0, 1;
	barrier;
	cbr_b1	$c0, @BB0_17;
	// BB#16:                                // %if.then181
	ld_group_align(8)_f64	$d1, [%__hsa_replaced_run_scratch][$s3];
	shl_u32	$s6, $s5, 3;
	ld_group_align(8)_f64	$d2, [%__hsa_replaced_run_scratch][$s6];
	cmp_lt_b1_f64	$c0, $d2, $d1;
	cvt_u64_b1	$d0, $c0;
	max_f64	$d1, $d2, $d1;
	st_group_align(8)_f64	$d1, [%__hsa_replaced_run_scratch][$s3];
	ld_group_align(4)_u32	$s6, [%__hsa_replaced_run_scratch_index][$s2];
	shl_u32	$s5, $s5, 2;
	ld_group_align(4)_u32	$s5, [%__hsa_replaced_run_scratch_index][$s5];
	{
		arg_u32 %_Z6selectiil;
		arg_u32 %__param_p0;
		arg_u32 %__param_p1;
		arg_u64 %__param_p2;
		st_arg_align(4)_u32	$s5, [%__param_p0];
		st_arg_align(4)_u32	$s6, [%__param_p1];
		st_arg_align(8)_u64	$d0, [%__param_p2];
		call	&_Z6selectiil (%_Z6selectiil) (%__param_p0, %__param_p1, %__param_p2);
		ld_arg_align(4)_u32	$s5, [%_Z6selectiil];
	}
	st_group_align(4)_u32	$s5, [%__hsa_replaced_run_scratch_index][$s2];

@BB0_17:
	// %if.end202
	add_u32	$s5, $s1, 2;
	cmp_lt_b1_s32	$c0, $s5, $s4;
	cmp_lt_b1_s32	$c1, $s1, 2;
	and_b1	$c0, $c1, $c0;
	cmp_ne_b1_b1	$c0, $c0, 1;
	barrier;
	cbr_b1	$c0, @BB0_19;
	// BB#18:                                // %if.then209
	ld_group_align(8)_f64	$d1, [%__hsa_replaced_run_scratch][$s3];
	shl_u32	$s6, $s5, 3;
	ld_group_align(8)_f64	$d2, [%__hsa_replaced_run_scratch][$s6];
	cmp_lt_b1_f64	$c0, $d2, $d1;
	cvt_u64_b1	$d0, $c0;
	max_f64	$d1, $d2, $d1;
	st_group_align(8)_f64	$d1, [%__hsa_replaced_run_scratch][$s3];
	ld_group_align(4)_u32	$s6, [%__hsa_replaced_run_scratch_index][$s2];
	shl_u32	$s5, $s5, 2;
	ld_group_align(4)_u32	$s5, [%__hsa_replaced_run_scratch_index][$s5];
	{
		arg_u32 %_Z6selectiil;
		arg_u32 %__param_p0;
		arg_u32 %__param_p1;
		arg_u64 %__param_p2;
		st_arg_align(4)_u32	$s5, [%__param_p0];
		st_arg_align(4)_u32	$s6, [%__param_p1];
		st_arg_align(8)_u64	$d0, [%__param_p2];
		call	&_Z6selectiil (%_Z6selectiil) (%__param_p0, %__param_p1, %__param_p2);
		ld_arg_align(4)_u32	$s5, [%_Z6selectiil];
	}
	st_group_align(4)_u32	$s5, [%__hsa_replaced_run_scratch_index][$s2];

@BB0_19:
	// %if.end230
	add_u32	$s5, $s1, 1;
	cmp_lt_b1_s32	$c0, $s5, $s4;
	cmp_lt_b1_s32	$c1, $s1, 1;
	and_b1	$c0, $c1, $c0;
	cmp_ne_b1_b1	$c0, $c0, 1;
	barrier;
	cbr_b1	$c0, @BB0_21;
	// BB#20:                                // %if.then237
	ld_group_align(8)_f64	$d1, [%__hsa_replaced_run_scratch][$s3];
	shl_u32	$s4, $s5, 3;
	ld_group_align(8)_f64	$d2, [%__hsa_replaced_run_scratch][$s4];
	cmp_lt_b1_f64	$c0, $d2, $d1;
	cvt_u64_b1	$d0, $c0;
	max_f64	$d1, $d2, $d1;
	st_group_align(8)_f64	$d1, [%__hsa_replaced_run_scratch][$s3];
	ld_group_align(4)_u32	$s3, [%__hsa_replaced_run_scratch_index][$s2];
	shl_u32	$s4, $s5, 2;
	ld_group_align(4)_u32	$s4, [%__hsa_replaced_run_scratch_index][$s4];
	{
		arg_u32 %_Z6selectiil;
		arg_u32 %__param_p0;
		arg_u32 %__param_p1;
		arg_u64 %__param_p2;
		st_arg_align(4)_u32	$s4, [%__param_p0];
		st_arg_align(4)_u32	$s3, [%__param_p1];
		st_arg_align(8)_u64	$d0, [%__param_p2];
		call	&_Z6selectiil (%_Z6selectiil) (%__param_p0, %__param_p1, %__param_p2);
		ld_arg_align(4)_u32	$s3, [%_Z6selectiil];
	}
	st_group_align(4)_u32	$s3, [%__hsa_replaced_run_scratch_index][$s2];

@BB0_21:
	// %if.end258
	barrier;
	cmp_ne_b1_s32	$c0, $s1, 0;
	cbr_b1	$c0, @BB0_23;
	// BB#22:                                // %if.then261
	cvt_u64_u32	$d0, $s0;
	ld_kernarg_align(8)_width(all)_u64	$d1, [%result];
	shl_u64	$d0, $d0, 2;
	add_u64	$d0, $d1, $d0;
	ld_group_align(4)_width(WAVESIZE)_u32	$s0, [%__hsa_replaced_run_scratch_index];
	st_global_align(4)_u32	$s0, [$d0];

@BB0_23:
	// %if.end264
	ret;
};
